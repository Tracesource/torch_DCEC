Training the 'CAE_3' architecture

The following parameters are used:
Batch size:	256
Number of workers:	4
Learning rate:	0.001
Pretraining learning rate:	0.001
Weight decay:	0.0
Pretraining weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Pretraining scheduler steps:	200
Pretraining scheduler gamma:	0.1
Number of epochs of training:	1000
Number of epochs of pretraining:	300
Clustering loss weight:	0.1
Update interval for target distribution:	80
Stop criterium tolerance:	0.01
Number of clusters:	10
Leaky relu:	True
Leaky slope:	0.01
Activations:	False
Bias:	True

Data preparation
Reading data from: MNIST train dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/300
----------
Pretraining:	Epoch: [1][10/235]	Loss 0.0735 (0.0880)	
Pretraining:	Epoch: [1][20/235]	Loss 0.0675 (0.0785)	
Pretraining:	Epoch: [1][30/235]	Loss 0.0608 (0.0735)	
Pretraining:	Epoch: [1][40/235]	Loss 0.0623 (0.0711)	
Pretraining:	Epoch: [1][50/235]	Loss 0.0590 (0.0688)	
