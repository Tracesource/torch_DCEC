Training the 'CAE_3' architecture

The following parameters are used:
Batch size:	256
Number of workers:	4
Learning rate:	0.001
Pretraining learning rate:	0.001
Weight decay:	0.0
Pretraining weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Pretraining scheduler steps:	200
Pretraining scheduler gamma:	0.1
Number of epochs of training:	1000
Number of epochs of pretraining:	300
Clustering loss weight:	0.1
Update interval for target distribution:	80
Stop criterium tolerance:	0.01
Number of clusters:	10
Leaky relu:	True
Leaky slope:	0.01
Activations:	False
Bias:	True

Data preparation
Reading data from: MNIST train dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/300
----------
Pretraining:	Epoch: [1][10/235]	Loss 0.0764 (0.0907)	
Pretraining:	Epoch: [1][20/235]	Loss 0.0698 (0.0810)	
Pretraining:	Epoch: [1][30/235]	Loss 0.0615 (0.0757)	
Pretraining:	Epoch: [1][40/235]	Loss 0.0627 (0.0729)	
Pretraining:	Epoch: [1][50/235]	Loss 0.0605 (0.0704)	
Pretraining:	Epoch: [1][60/235]	Loss 0.0508 (0.0679)	
Pretraining:	Epoch: [1][70/235]	Loss 0.0450 (0.0653)	
Pretraining:	Epoch: [1][80/235]	Loss 0.0439 (0.0629)	
Pretraining:	Epoch: [1][90/235]	Loss 0.0431 (0.0607)	
Pretraining:	Epoch: [1][100/235]	Loss 0.0355 (0.0586)	
Pretraining:	Epoch: [1][110/235]	Loss 0.0327 (0.0565)	
Pretraining:	Epoch: [1][120/235]	Loss 0.0335 (0.0547)	
Pretraining:	Epoch: [1][130/235]	Loss 0.0328 (0.0530)	
Pretraining:	Epoch: [1][140/235]	Loss 0.0304 (0.0515)	
Pretraining:	Epoch: [1][150/235]	Loss 0.0305 (0.0501)	
Pretraining:	Epoch: [1][160/235]	Loss 0.0274 (0.0488)	
Pretraining:	Epoch: [1][170/235]	Loss 0.0264 (0.0476)	
Pretraining:	Epoch: [1][180/235]	Loss 0.0274 (0.0464)	
Pretraining:	Epoch: [1][190/235]	Loss 0.0272 (0.0454)	
Pretraining:	Epoch: [1][200/235]	Loss 0.0255 (0.0445)	
Pretraining:	Epoch: [1][210/235]	Loss 0.0251 (0.0436)	
Pretraining:	Epoch: [1][220/235]	Loss 0.0244 (0.0427)	
Pretraining:	Epoch: [1][230/235]	Loss 0.0239 (0.0419)	
Pretraining:	 Loss: 0.0415

Pretraining:	Epoch 2/300
----------
Pretraining:	Epoch: [2][10/235]	Loss 0.0223 (0.0232)	
Pretraining:	Epoch: [2][20/235]	Loss 0.0227 (0.0232)	
Pretraining:	Epoch: [2][30/235]	Loss 0.0222 (0.0230)	
Pretraining:	Epoch: [2][40/235]	Loss 0.0223 (0.0230)	
Pretraining:	Epoch: [2][50/235]	Loss 0.0246 (0.0228)	
Pretraining:	Epoch: [2][60/235]	Loss 0.0223 (0.0228)	
Pretraining:	Epoch: [2][70/235]	Loss 0.0210 (0.0226)	
Pretraining:	Epoch: [2][80/235]	Loss 0.0199 (0.0224)	
Pretraining:	Epoch: [2][90/235]	Loss 0.0217 (0.0223)	
Pretraining:	Epoch: [2][100/235]	Loss 0.0193 (0.0222)	
Pretraining:	Epoch: [2][110/235]	Loss 0.0196 (0.0221)	
Pretraining:	Epoch: [2][120/235]	Loss 0.0202 (0.0220)	
Pretraining:	Epoch: [2][130/235]	Loss 0.0203 (0.0219)	
Pretraining:	Epoch: [2][140/235]	Loss 0.0193 (0.0217)	
Pretraining:	Epoch: [2][150/235]	Loss 0.0211 (0.0217)	
Pretraining:	Epoch: [2][160/235]	Loss 0.0185 (0.0216)	
Pretraining:	Epoch: [2][170/235]	Loss 0.0186 (0.0215)	
Pretraining:	Epoch: [2][180/235]	Loss 0.0205 (0.0214)	
Pretraining:	Epoch: [2][190/235]	Loss 0.0200 (0.0213)	
Pretraining:	Epoch: [2][200/235]	Loss 0.0187 (0.0212)	
Pretraining:	Epoch: [2][210/235]	Loss 0.0192 (0.0211)	
Pretraining:	Epoch: [2][220/235]	Loss 0.0190 (0.0210)	
Pretraining:	Epoch: [2][230/235]	Loss 0.0188 (0.0209)	
Pretraining:	 Loss: 0.0209

Pretraining:	Epoch 3/300
----------
Pretraining:	Epoch: [3][10/235]	Loss 0.0173 (0.0185)	
Pretraining:	Epoch: [3][20/235]	Loss 0.0186 (0.0185)	
Pretraining:	Epoch: [3][30/235]	Loss 0.0181 (0.0185)	
Pretraining:	Epoch: [3][40/235]	Loss 0.0188 (0.0186)	
Pretraining:	Epoch: [3][50/235]	Loss 0.0200 (0.0184)	
Pretraining:	Epoch: [3][60/235]	Loss 0.0186 (0.0185)	
Pretraining:	Epoch: [3][70/235]	Loss 0.0177 (0.0185)	
Pretraining:	Epoch: [3][80/235]	Loss 0.0165 (0.0184)	
Pretraining:	Epoch: [3][90/235]	Loss 0.0189 (0.0184)	
Pretraining:	Epoch: [3][100/235]	Loss 0.0170 (0.0184)	
Pretraining:	Epoch: [3][110/235]	Loss 0.0173 (0.0183)	
Pretraining:	Epoch: [3][120/235]	Loss 0.0178 (0.0183)	
Pretraining:	Epoch: [3][130/235]	Loss 0.0178 (0.0183)	
Pretraining:	Epoch: [3][140/235]	Loss 0.0170 (0.0183)	
Pretraining:	Epoch: [3][150/235]	Loss 0.0189 (0.0183)	
Pretraining:	Epoch: [3][160/235]	Loss 0.0160 (0.0183)	
Pretraining:	Epoch: [3][170/235]	Loss 0.0164 (0.0182)	
Pretraining:	Epoch: [3][180/235]	Loss 0.0183 (0.0182)	
Pretraining:	Epoch: [3][190/235]	Loss 0.0180 (0.0182)	
Pretraining:	Epoch: [3][200/235]	Loss 0.0165 (0.0181)	
Pretraining:	Epoch: [3][210/235]	Loss 0.0171 (0.0181)	
Pretraining:	Epoch: [3][220/235]	Loss 0.0171 (0.0180)	
Pretraining:	Epoch: [3][230/235]	Loss 0.0169 (0.0180)	
Pretraining:	 Loss: 0.0179

Pretraining:	Epoch 4/300
----------
Pretraining:	Epoch: [4][10/235]	Loss 0.0155 (0.0165)	
Pretraining:	Epoch: [4][20/235]	Loss 0.0168 (0.0166)	
Pretraining:	Epoch: [4][30/235]	Loss 0.0163 (0.0166)	
Pretraining:	Epoch: [4][40/235]	Loss 0.0171 (0.0168)	
Pretraining:	Epoch: [4][50/235]	Loss 0.0182 (0.0167)	
Pretraining:	Epoch: [4][60/235]	Loss 0.0168 (0.0167)	
Pretraining:	Epoch: [4][70/235]	Loss 0.0163 (0.0168)	
Pretraining:	Epoch: [4][80/235]	Loss 0.0150 (0.0167)	
Pretraining:	Epoch: [4][90/235]	Loss 0.0171 (0.0167)	
Pretraining:	Epoch: [4][100/235]	Loss 0.0158 (0.0167)	
Pretraining:	Epoch: [4][110/235]	Loss 0.0157 (0.0167)	
Pretraining:	Epoch: [4][120/235]	Loss 0.0160 (0.0167)	
Pretraining:	Epoch: [4][130/235]	Loss 0.0163 (0.0167)	
Pretraining:	Epoch: [4][140/235]	Loss 0.0155 (0.0166)	
Pretraining:	Epoch: [4][150/235]	Loss 0.0176 (0.0167)	
Pretraining:	Epoch: [4][160/235]	Loss 0.0148 (0.0167)	
Pretraining:	Epoch: [4][170/235]	Loss 0.0152 (0.0166)	
Pretraining:	Epoch: [4][180/235]	Loss 0.0170 (0.0166)	
Pretraining:	Epoch: [4][190/235]	Loss 0.0169 (0.0166)	
Pretraining:	Epoch: [4][200/235]	Loss 0.0153 (0.0166)	
Pretraining:	Epoch: [4][210/235]	Loss 0.0158 (0.0166)	
Pretraining:	Epoch: [4][220/235]	Loss 0.0161 (0.0165)	
Pretraining:	Epoch: [4][230/235]	Loss 0.0157 (0.0165)	
Pretraining:	 Loss: 0.0165

Pretraining:	Epoch 5/300
----------
Pretraining:	Epoch: [5][10/235]	Loss 0.0144 (0.0154)	
Pretraining:	Epoch: [5][20/235]	Loss 0.0157 (0.0155)	
Pretraining:	Epoch: [5][30/235]	Loss 0.0152 (0.0155)	
Pretraining:	Epoch: [5][40/235]	Loss 0.0163 (0.0157)	
Pretraining:	Epoch: [5][50/235]	Loss 0.0177 (0.0157)	
