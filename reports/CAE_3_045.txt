Training the 'CAE_3' architecture

The following parameters are used:
Batch size:	256
Number of workers:	4
Learning rate:	0.001
Pretraining learning rate:	0.001
Weight decay:	0.0
Pretraining weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Pretraining scheduler steps:	200
Pretraining scheduler gamma:	0.1
Number of epochs of training:	1000
Number of epochs of pretraining:	300
Clustering loss weight:	0.1
Update interval for target distribution:	80
Stop criterium tolerance:	0.01
Number of clusters:	10
Leaky relu:	True
Leaky slope:	0.01
Activations:	False
Bias:	True

Data preparation
Reading data from: MNIST train dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/300
----------
Pretraining:	Epoch: [1][10/235]	Loss 0.0936 (0.1196)	
Pretraining:	Epoch: [1][20/235]	Loss 0.0736 (0.1004)	
Pretraining:	Epoch: [1][30/235]	Loss 0.0645 (0.0901)	
Pretraining:	Epoch: [1][40/235]	Loss 0.0674 (0.0849)	
Pretraining:	Epoch: [1][50/235]	Loss 0.0675 (0.0812)	
Pretraining:	Epoch: [1][60/235]	Loss 0.0610 (0.0787)	
Pretraining:	Epoch: [1][70/235]	Loss 0.0582 (0.0764)	
Pretraining:	Epoch: [1][80/235]	Loss 0.0616 (0.0748)	
Pretraining:	Epoch: [1][90/235]	Loss 0.0614 (0.0732)	
Pretraining:	Epoch: [1][100/235]	Loss 0.0546 (0.0716)	
Pretraining:	Epoch: [1][110/235]	Loss 0.0476 (0.0697)	
Pretraining:	Epoch: [1][120/235]	Loss 0.0439 (0.0678)	
Pretraining:	Epoch: [1][130/235]	Loss 0.0431 (0.0659)	
Pretraining:	Epoch: [1][140/235]	Loss 0.0389 (0.0641)	
Pretraining:	Epoch: [1][150/235]	Loss 0.0377 (0.0624)	
Pretraining:	Epoch: [1][160/235]	Loss 0.0347 (0.0607)	
Pretraining:	Epoch: [1][170/235]	Loss 0.0331 (0.0592)	
Pretraining:	Epoch: [1][180/235]	Loss 0.0326 (0.0578)	
Pretraining:	Epoch: [1][190/235]	Loss 0.0321 (0.0564)	
Pretraining:	Epoch: [1][200/235]	Loss 0.0296 (0.0552)	
Pretraining:	Epoch: [1][210/235]	Loss 0.0294 (0.0539)	
Pretraining:	Epoch: [1][220/235]	Loss 0.0280 (0.0528)	
Pretraining:	Epoch: [1][230/235]	Loss 0.0276 (0.0517)	
Pretraining:	 Loss: 0.0513

Pretraining:	Epoch 2/300
----------
Pretraining:	Epoch: [2][10/235]	Loss 0.0260 (0.0266)	
Pretraining:	Epoch: [2][20/235]	Loss 0.0260 (0.0266)	
Pretraining:	Epoch: [2][30/235]	Loss 0.0259 (0.0263)	
Pretraining:	Epoch: [2][40/235]	Loss 0.0264 (0.0262)	
Pretraining:	Epoch: [2][50/235]	Loss 0.0282 (0.0260)	
Pretraining:	Epoch: [2][60/235]	Loss 0.0251 (0.0260)	
Pretraining:	Epoch: [2][70/235]	Loss 0.0236 (0.0258)	
Pretraining:	Epoch: [2][80/235]	Loss 0.0226 (0.0255)	
Pretraining:	Epoch: [2][90/235]	Loss 0.0242 (0.0254)	
Pretraining:	Epoch: [2][100/235]	Loss 0.0214 (0.0252)	
Pretraining:	Epoch: [2][110/235]	Loss 0.0214 (0.0250)	
Pretraining:	Epoch: [2][120/235]	Loss 0.0231 (0.0249)	
Pretraining:	Epoch: [2][130/235]	Loss 0.0221 (0.0247)	
Pretraining:	Epoch: [2][140/235]	Loss 0.0219 (0.0246)	
Pretraining:	Epoch: [2][150/235]	Loss 0.0231 (0.0245)	
Pretraining:	Epoch: [2][160/235]	Loss 0.0209 (0.0243)	
Pretraining:	Epoch: [2][170/235]	Loss 0.0210 (0.0242)	
Pretraining:	Epoch: [2][180/235]	Loss 0.0227 (0.0241)	
Pretraining:	Epoch: [2][190/235]	Loss 0.0222 (0.0240)	
Pretraining:	Epoch: [2][200/235]	Loss 0.0207 (0.0239)	
Pretraining:	Epoch: [2][210/235]	Loss 0.0211 (0.0238)	
Pretraining:	Epoch: [2][220/235]	Loss 0.0209 (0.0236)	
Pretraining:	Epoch: [2][230/235]	Loss 0.0205 (0.0235)	
Pretraining:	 Loss: 0.0234

Pretraining:	Epoch 3/300
----------
