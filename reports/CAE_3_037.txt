Training the 'CAE_3' architecture

The following parameters are used:
Batch size:	256
Number of workers:	4
Learning rate:	0.001
Pretraining learning rate:	0.001
Weight decay:	0.0
Pretraining weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Pretraining scheduler steps:	200
Pretraining scheduler gamma:	0.1
Number of epochs of training:	1000
Number of epochs of pretraining:	300
Clustering loss weight:	0.1
Update interval for target distribution:	80
Stop criterium tolerance:	0.01
Number of clusters:	10
Leaky relu:	True
Leaky slope:	0.01
Activations:	False
Bias:	True

Data preparation
Reading data from: MNIST train dataset
Image size used:	28x28
Training set size:	60000

Performing calculations on:	cuda:0

Pretrained weights loaded from file: CAE_3_007_pretrained.pt

Initializing cluster centers based on K-means

Begin clusters training

Updating target distribution
NMI: 0.77906	ARI: 0.73769	Acc 0.84457

Epoch 1/1000
----------
Epoch: [1][10/235]	Loss 0.0204 (0.0223)	Loss_recovery 0.0077 (0.0095)	Loss clustering 0.0127 (0.0128)	
Epoch: [1][20/235]	Loss 0.0213 (0.0217)	Loss_recovery 0.0084 (0.0089)	Loss clustering 0.0129 (0.0128)	
Epoch: [1][30/235]	Loss 0.0201 (0.0213)	Loss_recovery 0.0073 (0.0085)	Loss clustering 0.0128 (0.0128)	
Epoch: [1][40/235]	Loss 0.0205 (0.0212)	Loss_recovery 0.0078 (0.0084)	Loss clustering 0.0127 (0.0128)	
Epoch: [1][50/235]	Loss 0.0207 (0.0210)	Loss_recovery 0.0080 (0.0082)	Loss clustering 0.0127 (0.0128)	
Epoch: [1][60/235]	Loss 0.0206 (0.0209)	Loss_recovery 0.0078 (0.0082)	Loss clustering 0.0128 (0.0128)	
Epoch: [1][70/235]	Loss 0.0202 (0.0209)	Loss_recovery 0.0076 (0.0081)	Loss clustering 0.0127 (0.0128)	
Epoch: [1][80/235]	Loss 0.0200 (0.0208)	Loss_recovery 0.0072 (0.0081)	Loss clustering 0.0127 (0.0128)	

Updating target distribution:
NMI: 0.77636	ARI: 0.73433	Acc 0.84265	
Label divergence 0.00765< tol 0.01
Reached tolerance threshold. Stopping training.
Training complete in 0m 22s
